---
title: "Human Activity Recognition"
author: "Yuqian Liu"
date: "November 21, 2015"
output: html_document
---

# Synopsis

# Data analysis

### Load packages and set seed
```{r message=FALSE, warning=FALSE}
library(caret)
library(rpart)
library(randomForest)
library(kernlab)
set.seed(1235)
```

### Load data
```{r cache = TRUE}
FileName1 <- "pml-train.csv"
if (!file.exists(FileName1))
{
  url <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
  download.file(url = url, destfile = FileName1, method = "curl")
}
FileName2 <- "pml-test.csv"
if (!file.exists(FileName2))
{
  url <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
  download.file(url = url, destfile = FileName2, method = "curl")
}
dataTrain <- read.csv("pml-train.csv")
dataValid <- read.csv("pml-test.csv")
```

### Explore data
The data set has 160 variables
```{r cache = TRUE}
dim(dataTrain); dim(dataValid)
```

```{r cache = TRUE, eval=FALSE}
summary(dataTrain)
```
The first several variables are not related to sensors. The last variable `classe` is the activity type.
```{r cache = TRUE}
head(colnames(dataTrain)); tail(colnames(dataTrain))
```
There are `NA` in the data.
```{r cache = TRUE}
table(complete.cases(dataTrain))
```

### Preprocess data
1. Choose sensor-related variables and the activity type variable.
```{r cache = TRUE}
sensorColumns = grep(pattern = "_belt|_arm|_dumbbell|_forearm", names(dataTrain))
dataMy1 = dataTrain[, c(sensorColumns,160)]; dim(dataMy1)
dataValidMy1 = dataValid[, c(sensorColumns,160)]; dim(dataValidMy1)
```
There are 152 sensor-related variables.
2. Remove near-zero-variance variables
```{r cache = TRUE}
nzv <- nearZeroVar(dataMy1, saveMetrics=TRUE)
dataMy <- dataMy1[!nzv$nzv]; dim(dataMy)
dataValidMy <- dataValidMy1[!nzv$nzv]; dim(dataValidMy)
```
There are 93 sensor-related variables left.

3. Remove NA
One way is to remove all columns that contain NA.
```{r cache = TRUE}
missingData = is.na(dataMy)
omitColumns = which(colSums(missingData) !=0)
dataFinal = dataMy[, -omitColumns]; dim(dataFinal)
colnames(dataFinal)
table(complete.cases(dataFinal))
```
There are no NA in the training set. Next remove the same column in the validating set.
```{r cache = TRUE}
dataValidFinal = dataValidMy[, -omitColumns]
dim(dataValidFinal)
table(complete.cases(dataValidFinal))
```

An alternative way is to remove all rows that contain NA.
```{r cache = TRUE}
omitRows=which(rowSums(missingData) !=0)
dataFinal2=dataMy[-omitRows, ]
dim(dataFinal2)
```
However, there are only 406 rows left, which means the data set is too small to be applied the machine learning technique. Thus, we use the first way to remove NA.

4. Make sure the sensor related variables are numeric variables and the activity type variable is a factor variable.
```{r cache = TRUE}
class(dataFinal$classe)
table(sapply(dataFinal, is.numeric))
```

5. Check if there are similar number of rows for each activity type. For four types of sensor, check if the number of variables that relate to each type of sensor are similar.
```{r cache = TRUE}
table(dataFinal$classe)
length(grep(pattern = "_belt", names(dataFinal)))
length(grep(pattern = "_arm", names(dataFinal)))
length(grep(pattern = "_dumbbell", names(dataFinal)))
length(grep(pattern = "_forearm", names(dataFinal)))
```
We can regard this data set as homogenous. 

### Train models to classify activity types
1. Split the training set above into training data and testing data. 
```{r cache = TRUE}
inTrain <- createDataPartition(y=dataFinal$classe, p=0.7, list=FALSE)
training <- dataFinal[inTrain,]
testing <- dataFinal[-inTrain,]
dim(training)
dim(testing)
```
2. 2 repeats of 3-Fold CV

```{r cache = TRUE}
controlRf <- trainControl(method="cv", number=3,repeats = 2)
```

3.1 decision tree
```{r cache = TRUE}
system.time(modelFitTree <- train(classe ~., data=training, preProcess=c("center","scale"), trControl=controlRf, method="rpart")) 
modelFitTree
modelFitTree$finalModel
predictTree=predict(modelFitTree,testing); confusionMatrix(testing$classe, predictTree)
```
No need to plot
3.2 rf
```{r cache = TRUE}
system.time(modelFitRf <- train(classe ~., data=training, preProcess=c("center","scale"),  trControl=controlRf,method="rf"))
modelFitRf
modelFitRf$finalModel
predictRf=predict(modelFitRf,testing); confusionMatrix(testing$classe, predictRf)
```

3.3 Boosting with trees
```{r cache = TRUE}
system.time(modelFitSgb <- train(classe ~., data=training, preProcess=c("center","scale"),  trControl=controlRf,method="gbm",verbose=FALSE)) 
modelFitSgb
modelFitSgb$finalModel
predictSgb=predict(modelFitSgb,testing); confusionMatrix(testing$classe, predictSgb)
```

3.4 SVM with linear kernel
```{r cache = TRUE}
system.time(modelFitSvm <- train(classe ~., data=training, preProcess=c("center","scale"),  trControl=controlRf,method="svmLinear")) 
modelFitSvm
modelFitSvm$finalModel
predictSvm=predict(modelFitSvm,testing); confusionMatrix(testing$classe, predictSvm)
```
4. Model selection

Using PCA 
```{r cache = TRUE, eval=FALSE,  echo=FALSE}
system.time(modelFitRf2 <- train(classe ~., data=training, preProcess="pca",  trControl=controlRf,method="rf")) 
modelFitRf2$finalModel
modelFitRf2
predictRf2=predict(modelFitRf2,testing)
confusionMatrix(testing$classe, predictRf2)

system.time(modelFitTree2 <- train(classe ~., data=training, preProcess="pca",  trControl=controlRf,method="rpart")) 
modelFitTree2$finalModel
modelFitTree2
predictTree2=predict(modelFitTree2,testing)
confusionMatrix(testing$classe, predictTree2)

system.time(modelFitSgb2 <- train(classe ~., data=training, preProcess="pca",  trControl=controlRf,method="gbm",verbose=FALSE)) 
modelFitSgb2$finalModel
modelFitSgb2
predictSgb2=predict(modelFitSgb2,testing)
confusionMatrix(testing$classe, predictSgb2)

system.time(modelFitSvm2 <- train(classe ~., data=training, preProcess="pca",  trControl=controlRf,method="svmLinear")) 
modelFitSvm2$finalModel
modelFitSvm2
predictSvm2=predict(modelFitSvm2,testing)
confusionMatrix(testing$classe, predictSvm2)
```

### Optimize computational time
- I use `cache = TRUE` so that I only need to train my models once. 
- I tried to compute in parallel using the multiple cores of my laptop, but it didn't work with some methods I used to train my models. \
- I used principal component analysis (PCA) to reduce dimensions of data, which reduced the computational time. As excepted, the PCA preprocessing also reduced the accuracy of the predictions. 
Given the computational time in this project is not that large, there is no good reason to use PCA. Thus, I only use `cache = TRUE` in this project to optimize computational time.  

### Files to submit

```{r}
predictValid=predict(modelFitRf,dataValidFinal)
pml_write_files = function(x){
  n = length(x)
  for(i in 1:n){
    filename = paste0("problem_id_",i,".txt")
    write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
  }
}
pml_write_files(predictValid)

predictValid2=predict(modelFitSgb,dataValidFinal)
pml_write_files2 = function(x){
  n = length(x)
  for(i in 1:n){
    filename = paste0("problem_id2_",i,".txt")
    write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
  }
}
pml_write_files2(predictValid2)

```

# Summary

### Reference

